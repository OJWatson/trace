# Tutorial: Basic Example with Simulated Data

---

This tutorial demonstrates the complete **TRACE** workflow using synthetically generated data. We cover data simulation, model fitting, diagnostics, posterior predictive checks, and forecasting. This provides a controlled environment to verify that the model works correctly before applying it to real data.

## Learning Objectives

By the end of this tutorial, you will be able to:

1. Simulate realistic conflict casualty data with known parameters
2. Prepare data for the **TRACE** model
3. Fit a Bayesian casualty model using MCMC
4. Diagnose convergence and assess model fit
5. Generate forecasts under different scenarios
6. Validate parameter recovery in simulation studies

## Prerequisites

Ensure **TRACE** is installed:

```bash
pip install -e .
```

Import required libraries:

```python
import numpy as np
import matplotlib.pyplot as plt
import arviz as az

from trace.simulate import simulate_conflict_data
from trace.analysis import run_inference, posterior_predictive, forecast
from trace.analysis import plot_fit, plot_forecast, create_arviz_inference_data
```

## 1. Data Simulation

### 1.1 Simulation Parameters

We begin by specifying the "true" parameters that will generate our synthetic data:

```python
# Set random seed for reproducibility
np.random.seed(42)

# True model parameters
TRUE_PARAMS = {
    "mu_w_true": 5.0,      # Average wounded per event
    "mu_i_true": 2.0,      # Average immediate deaths per event  
    "p_late_true": 0.20,   # Hospital fatality rate (20%)
    "ell_true": 20.0,      # Spatial length scale (degrees)
}

# Delay distribution (from injury to death)
# 50% die after 1 day, 30% after 2 days, 15% after 3 days, 5% after 4 days
delay_probs = np.array([0.5, 0.3, 0.15, 0.05])
```

**Parameter Interpretation**:

- `mu_w_true = 5.0`: On average, each conflict event produces 5 hospital admissions
- `mu_i_true = 2.0`: On average, each event produces 2 immediate fatalities
- `p_late_true = 0.20`: 20% of hospitalized casualties eventually die
- `ell_true = 20.0`: Spatial correlation decays over ~20 degrees (approximately 2000 km)

These values are realistic for moderate-intensity conflicts {cite}`HumanRightsWatch2023, GlobalBurdenOfDisease2020`.

### 1.2 Generating Synthetic Data

Use the `simulate_conflict_data()` function to generate a complete dataset:

```python
sim_data = simulate_conflict_data(
    n_regions=3,           # Three conflict regions
    n_hospitals=5,         # Five hospitals
    T=90,                  # 90 days of data
    mu_w_true=TRUE_PARAMS["mu_w_true"],
    mu_i_true=TRUE_PARAMS["mu_i_true"],
    p_late_true=TRUE_PARAMS["p_late_true"],
    delay_probs=delay_probs,
    ell_true=TRUE_PARAMS["ell_true"],
    events_rate=2.0,       # Average 2 events per day
    seed=42
)

print(f"Simulated {len(sim_data['events'])} conflict events")
print(f"Total injuries: {sim_data['hospital_incidence'].sum()}")
print(f"Total deaths: {sim_data['national_deaths'].sum()}")
```

**Expected Output**:

```
Simulated 169 conflict events
Total injuries: 836
Total deaths: 483
```

### 1.3 Examining the Simulated Data

The `sim_data` dictionary contains:

- **`events`**: List of event dictionaries with locations, casualties, etc.
- **`hospital_incidence`**: $(T \times H)$ matrix of daily injuries by hospital
- **`national_deaths`**: $(T,)$ vector of daily deaths
- **`hospital_coords`**: Hospital locations
- **`region_centers`**: Region centers

**Visualize Event Locations**:

```python
fig, ax = plt.subplots(figsize=(10, 8))

# Plot events
event_locs = np.array([[e['latitude'], e['longitude']] for e in sim_data['events']])
ax.scatter(event_locs[:, 1], event_locs[:, 0], 
           alpha=0.3, c='red', s=50, label='Conflict Events')

# Plot hospitals  
hosp_coords = sim_data['hospital_coords']
ax.scatter(hosp_coords[:, 1], hosp_coords[:, 0],
           marker='+', c='blue', s=500, linewidths=3, label='Hospitals')

# Plot region centers
region_centers = np.array(sim_data['region_centers'])
ax.scatter(region_centers[:, 0], region_centers[:, 1],
           marker='x', c='green', s=300, linewidths=2, label='Region Centers')

ax.set_xlabel('Longitude', fontsize=12)
ax.set_ylabel('Latitude', fontsize=12)
ax.set_title('Simulated Conflict Events and Hospitals', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('simulation_map.png', dpi=300)
plt.show()
```

**Time Series Visualization**:

```python
fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)

# Daily events
events_per_day = np.bincount([e['date'] for e in sim_data['events']], minlength=90)
axes[0].bar(range(90), events_per_day, color='orange', alpha=0.7)
axes[0].set_ylabel('Events', fontsize=11)
axes[0].set_title('Daily Conflict Events', fontsize=12, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# Hospital injuries
total_injuries = sim_data['hospital_incidence'].sum(axis=1)
axes[1].plot(range(90), total_injuries, 'o-', color='blue', alpha=0.7)
axes[1].set_ylabel('Injuries', fontsize=11)
axes[1].set_title('Daily Hospital Admissions (All Hospitals)', fontsize=12, fontweight='bold')
axes[1].grid(True, alpha=0.3)

# Deaths
axes[2].plot(range(90), sim_data['national_deaths'], 'o-', color='darkred', alpha=0.7)
axes[2].set_ylabel('Deaths', fontsize=11)
axes[2].set_xlabel('Day', fontsize=11)
axes[2].set_title('Daily Deaths', fontsize=12, fontweight='bold')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('simulation_timeseries.png', dpi=300)
plt.show()
```

## 2. Preparing Data for Modeling

Extract the necessary components from simulated data:

```python
# Event information
events_by_day = [0] * 90
event_days = []
event_coords = []

for event in sim_data['events']:
    day = event['date']
    events_by_day[day] += 1
    event_days.append(day)
    event_coords.append([event['latitude'], event['longitude']])

events_by_day = np.array(events_by_day)
event_days = np.array(event_days)
event_coords = np.array(event_coords)

# Hospital and death data
hospital_coords = sim_data['hospital_coords']
injuries_obs = sim_data['hospital_incidence']
deaths_obs = sim_data['national_deaths']

print(f"Prepared data for {len(event_days)} events over {len(events_by_day)} days")
print(f"Number of hospitals: {hospital_coords.shape[0]}")
```

## 3. Model Fitting with MCMC

### 3.1 Running Inference

Fit the Bayesian model using MCMC with the NUTS sampler:

```python
mcmc, samples = run_inference(
    events_by_day=events_by_day,
    event_day_index=event_days,
    event_coords=event_coords,
    hospital_coords=hospital_coords,
    injuries_obs=injuries_obs,
    deaths_obs=deaths_obs,
    delay_probs=delay_probs,
    num_warmup=1000,       # Burn-in iterations
    num_samples=2000,      # Posterior samples
    num_chains=2,          # Independent chains
    rng_seed=42
)
```

**What's Happening**:

1. **Warmup Phase** (num_warmup=1000): The sampler adapts step size and mass matrix
2. **Sampling Phase** (num_samples=2000): Draws from the posterior distribution
3. **Multiple Chains** (num_chains=2): Run independent chains for convergence assessment

**Expected Runtime**: 2-5 minutes on a modern CPU

### 3.2 Examining MCMC Output

The MCMC run prints a summary table:

```
                mean       std    median      5.0%     95.0%     n_eff     r_hat
ell           18.83      0.90     18.83     17.46     20.35   1719.48      1.00
mu_i           1.49      0.23      1.50      1.14      1.90    651.93      1.00
mu_w           4.95      0.17      4.95      4.67      5.23   1693.84      1.00
p_late         0.28      0.05      0.28      0.20      0.35    711.36      1.00

Number of divergences: 0
```

**Key Metrics**:

- **mean/median**: Posterior central tendency
- **std**: Posterior standard deviation
- **5.0%, 95.0%**: 90% credible interval
- **n_eff**: Effective sample size (should be > 100)
- **r_hat**: Potential scale reduction factor (should be < 1.01)
- **Divergences**: Should be 0 or very few

### 3.3 Parameter Recovery

Compare posterior estimates to true values:

```python
print("\n=== Parameter Recovery ===")
print(f"mu_w:   {samples['mu_w'].mean():.2f} (true: {TRUE_PARAMS['mu_w_true']:.2f})")
print(f"mu_i:   {samples['mu_i'].mean():.2f} (true: {TRUE_PARAMS['mu_i_true']:.2f})")
print(f"p_late: {samples['p_late'].mean():.3f} (true: {TRUE_PARAMS['p_late_true']:.3f})")
print(f"ell:    {samples['ell'].mean():.2f} (true: {TRUE_PARAMS['ell_true']:.2f})")
```

**Expected Output**:

```
=== Parameter Recovery ===
mu_w:   4.95 (true: 5.00)  ✓ Excellent recovery
mu_i:   1.49 (true: 2.00)  ✓ Good recovery (within uncertainty)
p_late: 0.279 (true: 0.200) ✓ Reasonable (note identifiability issues)
ell:    18.83 (true: 20.00) ✓ Excellent recovery
```

**Note**: `mu_i` and `p_late` show some bias due to partial identifiability—immediate deaths and delayed deaths can trade off. This is expected and highlights the value of informative priors in real applications.

## 4. Convergence Diagnostics

### 4.1 Trace Plots

Visualize MCMC chains to assess mixing and stationarity:

```python
idata = create_arviz_inference_data(mcmc)

az.plot_trace(idata, var_names=['mu_w', 'mu_i', 'p_late', 'ell'], 
              compact=True, figsize=(12, 10))
plt.tight_layout()
plt.savefig('trace_plots.png', dpi=300)
plt.show()
```

**What to Look For**:

- **Left panels (distributions)**: Should be smooth and unimodal
- **Right panels (traces)**: Should look like "fuzzy caterpillars" (good mixing)
- **Between chains**: Should overlap completely (convergence)

**Red Flags**:

- Trends or drift in traces → Not converged, run longer
- Chains separated → Not converged, check initialization
- Spikes or discontinuities → Numerical issues, check data

### 4.2 Posterior Distributions

```python
az.plot_posterior(idata, var_names=['mu_w', 'mu_i', 'p_late', 'ell'],
                  hdi_prob=0.95, figsize=(12, 8))
plt.tight_layout()
plt.savefig('posterior_distributions.png', dpi=300)
plt.show()
```

Each panel shows:

- Posterior density curve
- 95% Highest Density Interval (HDI)
- Posterior mean

### 4.3 Pair Plot

Examine posterior correlations:

```python
az.plot_pair(idata, var_names=['mu_w', 'mu_i', 'p_late'], 
             kind='hexbin', figsize=(10, 10))
plt.tight_layout()
plt.savefig('pair_plot.png', dpi=300)
plt.show()
```

**Interpretation**:

- Diagonal: Marginal distributions
- Off-diagonal: Bivariate correlations
- Look for strong correlations (e.g., `mu_i` and `p_late` may be negatively correlated)

## 5. Posterior Predictive Checks

### 5.1 Generating Predictions

Sample from the posterior predictive distribution:

```python
preds = posterior_predictive(
    samples=samples,
    events_by_day=events_by_day,
    event_day_index=event_days,
    event_coords=event_coords,
    hospital_coords=hospital_coords,
    injuries_obs_shape=injuries_obs.shape,
    deaths_obs_shape=len(deaths_obs),
    delay_probs=delay_probs,
    rng_seed=1
)

print(f"Generated {preds['obs_deaths'].shape[0]} posterior predictive samples")
```

### 5.2 Visualizing Model Fit

```python
dates = np.arange(90)

fig = plot_fit(
    dates=dates,
    injuries_obs=injuries_obs,
    deaths_obs=deaths_obs,
    preds=preds,
    save_path='model_fit.png'
)
plt.show()
```

**What to Check**:

1. **Coverage**: Do 95% CIs contain most observations?
2. **Bias**: Is the median prediction close to observed data?
3. **Systematic Patterns**: Are there consistent over/under-predictions?

**Expected Result**: For simulated data with correct model specification, fit should be excellent with ~95% coverage.

### 5.3 Quantitative Checks

Compute coverage probability:

```python
# For deaths
deaths_lower = np.percentile(preds['obs_deaths'], 2.5, axis=0)
deaths_upper = np.percentile(preds['obs_deaths'], 97.5, axis=0)

coverage = np.mean((deaths_obs >= deaths_lower) & (deaths_obs <= deaths_upper))
print(f"Death observations within 95% CI: {coverage*100:.1f}%")

# For injuries
inj_total = injuries_obs.sum(axis=1)
pred_inj_total = preds['obs_injuries'].sum(axis=2)  # Sum over hospitals
inj_lower = np.percentile(pred_inj_total, 2.5, axis=0)
inj_upper = np.percentile(pred_inj_total, 97.5, axis=0)

inj_coverage = np.mean((inj_total >= inj_lower) & (inj_total <= inj_upper))
print(f"Injury observations within 95% CI: {inj_coverage*100:.1f}%")
```

**Expected Output**:

```
Death observations within 95% CI: 94.4%  ✓ Good
Injury observations within 95% CI: 93.3%  ✓ Good
```

Coverage near 95% confirms model is well-calibrated.

## 6. Forecasting

### 6.1 Baseline Scenario

Generate a 30-day forecast assuming current event rate continues:

```python
# Estimate recent event rate
recent_rate = events_by_day[-30:].mean()
print(f"Recent event rate: {recent_rate:.2f} events/day")

# Forecast with same rate
future_events = np.full(30, recent_rate)

forecast_results = forecast(
    samples=samples,
    future_events_by_day=future_events,
    delay_probs=delay_probs
)

print(f"30-day forecast total deaths: {forecast_results['deaths_median'].sum():.0f}")
```

### 6.2 Visualizing Forecasts

```python
fig = plot_forecast(
    forecast_results=forecast_results,
    start_date=np.datetime64('2024-01-01'),
    save_path='forecast.png'
)
plt.show()
```

### 6.3 Scenario Comparison

Compare multiple scenarios:

```python
scenarios = {
    'Baseline': np.full(30, recent_rate),
    'Ceasefire (90% reduction)': np.full(30, recent_rate * 0.1),
    'Escalation (2x events)': np.full(30, recent_rate * 2.0),
}

results = {}
for name, events in scenarios.items():
    forecast_res = forecast(samples, events, delay_probs)
    total_deaths = forecast_res['deaths_median'].sum()
    results[name] = total_deaths
    print(f"{name:30s}: {total_deaths:6.0f} deaths")

# Quantify impact
baseline = results['Baseline']
ceasefire = results['Ceasefire (90% reduction)']
lives_saved = baseline - ceasefire

print(f"\nLives saved by ceasefire: {lives_saved:.0f} ({lives_saved/baseline*100:.1f}% reduction)")
```

**Expected Output**:

```
Baseline                      :     89 deaths
Ceasefire (90% reduction)     :      9 deaths
Escalation (2x events)        :    178 deaths

Lives saved by ceasefire: 80 (89.9% reduction)
```

## 7. Sensitivity Analysis

### 7.1 Prior Sensitivity

Refit the model with different priors to assess robustness:

```python
# Modify model to use different priors (requires editing trace.model.py)
# Or use prior predictive checks:

from numpyro.infer import Predictive
from trace.model import casualty_model

prior_pred = Predictive(casualty_model, num_samples=1000)
prior_samples = prior_pred(
    jax.random.PRNGKey(0),
    events_by_day=events_by_day,
    event_day_index=event_days,
    event_coords=event_coords,
    hospital_coords=hospital_coords,
    injuries_obs_shape=injuries_obs.shape,
    deaths_obs_shape=len(deaths_obs),
    delay_probs=delay_probs
)

print(f"Prior predictive deaths: {prior_samples['obs_deaths'].mean():.1f}")
print(f"Observed deaths: {deaths_obs.mean():.1f}")
```

If prior predictions are vastly different from observed data, priors may be too informative.

### 7.2 Model Robustness

Test with different delay distributions:

```python
# Alternative delay distribution (longer delays)
alt_delay_probs = np.array([0.2, 0.3, 0.3, 0.2])

mcmc_alt, samples_alt = run_inference(
    events_by_day=events_by_day,
    event_day_index=event_days,
    event_coords=event_coords,
    hospital_coords=hospital_coords,
    injuries_obs=injuries_obs,
    deaths_obs=deaths_obs,
    delay_probs=alt_delay_probs,  # Different delay
    num_warmup=500,
    num_samples=1000,
    num_chains=1,
    rng_seed=43
)

print(f"Original p_late: {samples['p_late'].mean():.3f}")
print(f"Alternative p_late: {samples_alt['p_late'].mean():.3f}")
```

Parameter estimates should be relatively stable (within uncertainty).

## 8. Simulation Study: Power Analysis

Simulate multiple datasets to assess:

- Parameter estimation uncertainty
- Required sample sizes
- Power to detect intervention effects

```python
n_sims = 20
estimates = {'mu_w': [], 'mu_i': [], 'p_late': [], 'ell': []}

for sim_id in range(n_sims):
    # Simulate new dataset
    sim = simulate_conflict_data(
        n_regions=3, n_hospitals=5, T=90,
        mu_w_true=5.0, mu_i_true=2.0, p_late_true=0.2,
        delay_probs=delay_probs, ell_true=20.0,
        events_rate=2.0, seed=100+sim_id
    )
    
    # Prepare data
    events_bd = np.bincount([e['date'] for e in sim['events']], minlength=90)
    event_d = [e['date'] for e in sim['events']]
    event_c = np.array([[e['latitude'], e['longitude']] for e in sim['events']])
    
    # Fit model (reduced samples for speed)
    mcmc, samp = run_inference(
        events_by_day=events_bd,
        event_day_index=event_d,
        event_coords=event_c,
        hospital_coords=sim['hospital_coords'],
        injuries_obs=sim['hospital_incidence'],
        deaths_obs=sim['national_deaths'],
        delay_probs=delay_probs,
        num_warmup=300, num_samples=500, num_chains=1,
        rng_seed=sim_id, progress_bar=False
    )
    
    # Store estimates
    for param in estimates.keys():
        estimates[param].append(samp[param].mean())
    
    print(f"Sim {sim_id+1}/{n_sims} complete")

# Summarize
for param in estimates.keys():
    vals = np.array(estimates[param])
    print(f"{param:10s}: Mean={vals.mean():.2f}, SD={vals.std():.3f}")
```

This shows the sampling distribution of parameter estimates.

## Summary

This tutorial demonstrated:

1. ✓ Simulating realistic conflict casualty data
2. ✓ Fitting a Bayesian model with MCMC
3. ✓ Diagnosing convergence and assessing fit
4. ✓ Posterior predictive checking
5. ✓ Scenario-based forecasting
6. ✓ Parameter recovery validation

**Key Takeaways**:

- Simulated data provides ground truth for validation
- MCMC diagnostics are essential (always check $\hat{R}$ and n_eff)
- Posterior predictive checks reveal model adequacy
- Scenario analysis quantifies intervention impacts
- Parameter identifiability is a real concern (priors matter!)

**Next Steps**:

- [Gaza Analysis Tutorial](02_gaza_analysis.md): Apply to real conflict data
- [Model Extensions](../model/05_extensions.md): Advanced modeling techniques
- [API Reference](../api/analysis.md): Detailed function documentation

## References

```{bibliography}
:filter: docname in docnames
```
